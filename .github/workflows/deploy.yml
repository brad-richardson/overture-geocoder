name: Deploy

on:
  push:
    branches: [main]
  workflow_dispatch: # Manual trigger

jobs:
  test:
    uses: ./.github/workflows/ci.yml

  build-index:
    needs: test
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install DuckDB CLI
        run: |
          curl -LO https://github.com/duckdb/duckdb/releases/download/v1.1.3/duckdb_cli-linux-amd64.zip
          unzip duckdb_cli-linux-amd64.zip
          chmod +x duckdb
          sudo mv duckdb /usr/local/bin/

      - name: Install Python dependencies
        run: pip install duckdb

      - name: Download global divisions from Overture
        run: ./scripts/download_divisions.sh

      - name: Build divisions index
        run: |
          mkdir -p indexes
          python scripts/build_divisions_index.py

      - name: Upload index artifact
        uses: actions/upload-artifact@v4
        with:
          name: divisions-index
          path: indexes/divisions-global.db
          retention-days: 1

  deploy:
    needs: test  # Change to build-index when D1 upload is enabled
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "20"
          cache: "npm"

      - name: Install dependencies
        run: npm ci

      # - name: Download index artifact
      #   uses: actions/download-artifact@v4
      #   with:
      #     name: divisions-index
      #     path: indexes/
      #
      # - name: Set up Python
      #   uses: actions/setup-python@v5
      #   with:
      #     python-version: "3.11"

      # D1 database upload steps (uncomment when ready to deploy data)
      # - name: Export to chunked SQL files
      #   run: |
      #     mkdir -p exports/divisions
      #     python scripts/export_to_sql.py indexes/divisions-global.db exports/divisions --table divisions
      #
      # - name: Upload schema to D1
      #   run: npx wrangler d1 execute geocoder-divisions-global --remote --file=exports/divisions/schema.sql
      #   env:
      #     CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
      #     CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
      #
      # - name: Upload data chunks to D1
      #   run: |
      #     for file in exports/divisions/data-*.sql; do
      #       echo "Uploading $file..."
      #       npx wrangler d1 execute geocoder-divisions-global --remote --file="$file"
      #     done
      #   env:
      #     CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
      #     CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}

      - name: Deploy Worker
        run: npx wrangler deploy
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
